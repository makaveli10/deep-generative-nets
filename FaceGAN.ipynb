{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install tensorflow=='2.0.0'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.version","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom tensorflow.keras import layers\nimport time\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Navigate to images directory\nimport os\nimages = os.listdir(\"../input/celeba-dataset/img_align_celeba/img_align_celeba\")\ntrainData = []\nprint(type(images))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(300):\n    trainData.append(cv2.resize(\n        cv2.imread(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/\" + images[i]), (64,64)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData = np.float32((np.array(trainData) - 127.5) / 127.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(trainData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generator**\n\nThe generator uses upsampling (Conv2DTranspose) to produce an image from a seed(random noise). Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 64x64x3."},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(4*4*1024, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Reshape((4, 4, 1024)))\n    assert model.output_shape == (None, 4, 4, 1024)\n    \n    model.add(layers.Conv2DTranspose(512, (4,4), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    assert model.output_shape == (None, 8, 8, 512)\n    \n    model.add(layers.Conv2DTranspose(256, (4,4), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    assert model.output_shape == (None, 16, 16, 256)\n    \n    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    assert model.output_shape == (None, 32, 32, 128)\n    \n    model.add(layers.Conv2DTranspose(3,\n                                     (4,4),\n                                     strides=(2, 2),\n                                     padding='same',\n                                     use_bias=False,\n                                     activation='tanh'))\n    assert model.output_shape == (None, 64, 64, 3)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = generator_model()\ntf.keras.utils.plot_model(generator, show_shapes=True, to_file='generator.png')\n# gen_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"noise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Discriminator**\n\nThe discriminator is a CNN-based image classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(128,\n                            (4,4),\n                            strides=(2,2),\n                            padding='same',\n                            input_shape=[64, 64, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(256, (4,4), strides=(2,2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Conv2D(512, (4,4), strides=(2,2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Conv2D(1024, (4,4), strides=(2,2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(1, (4,4), strides=(1,1), padding='same'))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator = discriminator_model()\ntf.keras.utils.plot_model(discriminator, to_file='discriminator.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Discriminator Loss**\n\nThis method tells how well the discriminator is able to distinguish between real and generated images. It compares the discrimators's preds on real images to an array of 1's, and the predictions of fake(generated) images to an array of zeros."},{"metadata":{"trusted":true},"cell_type":"code","source":"# discriminator loss\ndef discriminator_loss(real_out, fake_out):\n    real_loss = cross_entropy(tf.ones_like(real_out), real_out)\n    fake_loss = cross_entropy(tf.zeros_like(fake_out), fake_out)\n    total_loss = real_loss + fake_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generator Loss**\n\nTHe generator loss tells how well it was able to fool the discriminator. If the generator is performing well then the discriminator will classify generated images as real. Here we will compare the discriminators decisions on the generated images to the array of 1's. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define separate optimizers for both networks\ngenerator_opt = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_opt = tf.keras.optimizers.Adam(1e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Save Checkpoints**\n\nIf the notebook/training is interrupted checkpoints can be restored."},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_opt=generator_opt,\n                                 discriminator_opt=discriminator_opt,\n                                 generator=generator,\n                                 discriminator=discriminator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 50\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# We will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([10, noise_dim])\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        \n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n        \n        g_loss = generator_loss(fake_output)\n        d_loss = discriminator_loss(real_output, fake_output)\n    \n    gen_grads = gen_tape.gradient(g_loss, generator.trainable_variables)\n    dis_grads = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n    \n    generator_opt.apply_gradients(zip(gen_grads, generator.trainable_variables))\n    discriminator_opt.apply_gradients(zip(dis_grads, discriminator.trainable_variables))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        \n        for i in range(30):\n            train_step(dataset[i:i+10])\n        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n    noise = tf.random.normal([10,100])\n    generator(noise, training=False)\n    generated = generator(noise, training=False)\n    tensor = generated[0,:,:,0]\n    tensorInArr = tf.keras.backend.get_value(tensor)\n    plt.imshow(tensorInArr, cmap='gray')\n#         print(\"epoch {}\".format(epoch))\n#         # Produce images for the GIF as we go\n#         display.clear_output(wait=True)\n#         generate_and_save_images(generator,\n#                              epoch + 1,\n#                              seed)\n\n#         # Save the model every 15 epochs\n#         if (epoch + 1) % 15 == 0:\n#             checkpoint.save(file_prefix = checkpoint_prefix)\n\n#         print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n#     # Generate after the final epoch\n#     display.clear_output(wait=True)\n#     generate_and_save_images(generator,\n#                            epochs,\n#                            seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)\n    fig = plt.figure(figsize=(4,4))   \n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n        plt.axis('off')\n  \n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(trainData, 25)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}